{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "185bf12a",
   "metadata": {},
   "source": [
    "### Import Required Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6559ba6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from Architectures.Basic_Sequence_classification import Sequence_Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6b8ed7",
   "metadata": {},
   "source": [
    "### Prepare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b7eecf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self,\n",
    "                vocab_size,\n",
    "                embed_dim=128,\n",
    "                num_layers=10,\n",
    "                num_heads=8,\n",
    "                ff_dim=512,\n",
    "                pre_normalization=True,\n",
    "                max_position_embeddings=128,\n",
    "                dropout_prob=0.1,\n",
    "                num_labels=6,\n",
    "                device = \"cuda\"):\n",
    "        self.device = device\n",
    "        self.vocab_size = vocab_size  # Tokenizer vocab size\n",
    "        self.embed_dim = embed_dim  # Embedding & input to attention\n",
    "        self.num_layers = num_layers  # Number of encoder layers\n",
    "        self.num_heads = num_heads  # Number of heads in Multi-Head Attention\n",
    "        self.ff_dim = ff_dim  # Feed Forward hidden dimension\n",
    "        self.pre_normalization = pre_normalization  # LayerNorm before or after attention/FFN\n",
    "        self.max_length = max_position_embeddings  # Max sequence length\n",
    "        self.dropout_prob = dropout_prob  # Dropout probability\n",
    "        self.num_classess = num_labels  # Output classes (for classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edbaff10",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d8bc6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequence_Classification(config=Config(vocab_size=30522)).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01be3dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3711/2926407311.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best_model.pt\"))\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "model.classification_head[3] = nn.Linear(in_features=128, out_features=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36beebb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence_Classification(\n",
      "  (position_embedding): SinusoidalEmbeddingLayer(\n",
      "    (embedding): Embedding(30522, 128)\n",
      "    (layer_norm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "  )\n",
      "  (encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-9): 10 x TransformerEncoderLayer(\n",
      "        (attn): MultiHeadAttention(\n",
      "          (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (value): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (output): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ff): FeedForward2(\n",
      "          (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu): GELU(approximate='none')\n",
      "        )\n",
      "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (final): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (classification_head): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21594b3a",
   "metadata": {},
   "source": [
    "### Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58805a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"hungnm/multilingual-amazon-review-sentiment-processed\", token=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe15b961",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_en = ds.filter(lambda example: example['language'] == 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe44afec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "MAX_LEN = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "619fe18f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74f4e4fb72a74d46b15c880037c1553c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7792 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['stars', 'text', 'language', 'label', 'len', 'valid', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 216904\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['stars', 'text', 'language', 'label', 'len', 'valid', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 7792\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['stars', 'text', 'language', 'label', 'len', 'valid', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 7811\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=MAX_LEN)\n",
    "\n",
    "encoded_dataset = ds_en.map(encode)\n",
    "ds_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83549af9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_dataset['test']['attention_mask'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "068fc362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['stars', 'text', 'language', 'label', 'len', 'valid', 'input_ids', 'attention_mask', 'token_type_ids'],\n",
       "        num_rows: 216904\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['stars', 'text', 'language', 'label', 'len', 'valid', 'input_ids', 'attention_mask', 'token_type_ids'],\n",
       "        num_rows: 7792\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['stars', 'text', 'language', 'label', 'len', 'valid', 'input_ids', 'attention_mask', 'token_type_ids'],\n",
       "        num_rows: 7811\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12c3519a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ab686c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Amazon_Dataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.data = dataset  # HuggingFace Dataset object\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]  # Access row as dict\n",
    "        return {\n",
    "            'input_ids': torch.tensor(item['input_ids']),\n",
    "            'attention_mask': torch.tensor(item['attention_mask']),\n",
    "            'labels': torch.tensor(item['label'])\n",
    "        }\n",
    "\n",
    "train_dataset = Amazon_Dataset(encoded_dataset['train'])\n",
    "val_dataset = Amazon_Dataset(encoded_dataset['validation'])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "699cbe73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7792"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82ae6f9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "244"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8a55d8",
   "metadata": {},
   "source": [
    "### Train and Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6930693f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d551165e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af59f6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 Loss: 0.3060\n",
      "Batch 100 Loss: 0.3129\n",
      "Batch 200 Loss: 0.3244\n",
      "Batch 300 Loss: 0.4366\n",
      "Batch 400 Loss: 0.3293\n",
      "Batch 500 Loss: 0.1957\n",
      "Batch 600 Loss: 0.5469\n",
      "Batch 700 Loss: 0.2543\n",
      "Batch 800 Loss: 0.4415\n",
      "Batch 900 Loss: 0.2342\n",
      "Batch 1000 Loss: 0.2883\n",
      "Batch 1100 Loss: 0.3154\n",
      "Batch 1200 Loss: 0.3352\n",
      "Batch 1300 Loss: 0.3172\n",
      "Batch 1400 Loss: 0.1305\n",
      "Batch 1500 Loss: 0.2240\n",
      "Batch 1600 Loss: 0.2258\n",
      "Batch 1700 Loss: 0.2931\n",
      "Batch 1800 Loss: 0.2104\n",
      "Batch 1900 Loss: 0.4190\n",
      "Batch 2000 Loss: 0.1664\n",
      "Batch 2100 Loss: 0.4007\n",
      "Batch 2200 Loss: 0.2938\n",
      "Batch 2300 Loss: 0.1625\n",
      "Batch 2400 Loss: 0.2281\n",
      "Batch 2500 Loss: 0.2306\n",
      "Batch 2600 Loss: 0.1883\n",
      "Batch 2700 Loss: 0.3605\n",
      "Batch 2800 Loss: 0.1346\n",
      "Batch 2900 Loss: 0.1912\n",
      "Batch 3000 Loss: 0.2180\n",
      "Batch 3100 Loss: 0.2077\n",
      "Batch 3200 Loss: 0.1682\n",
      "Batch 3300 Loss: 0.2153\n",
      "Batch 3400 Loss: 0.3466\n",
      "Batch 3500 Loss: 0.2246\n",
      "Batch 3600 Loss: 0.2531\n",
      "Batch 3700 Loss: 0.2182\n",
      "Batch 3800 Loss: 0.3488\n",
      "Batch 3900 Loss: 0.4703\n",
      "Batch 4000 Loss: 0.2481\n",
      "Batch 4100 Loss: 0.2766\n",
      "Batch 4200 Loss: 0.1289\n",
      "Batch 4300 Loss: 0.4692\n",
      "Batch 4400 Loss: 0.1330\n",
      "Batch 4500 Loss: 0.3528\n",
      "Batch 4600 Loss: 0.1488\n",
      "Batch 4700 Loss: 0.2224\n",
      "Batch 4800 Loss: 0.2526\n",
      "Batch 4900 Loss: 0.0662\n",
      "Batch 5000 Loss: 0.1756\n",
      "Batch 5100 Loss: 0.3888\n",
      "Batch 5200 Loss: 0.3038\n",
      "Batch 5300 Loss: 0.2104\n",
      "Batch 5400 Loss: 0.1229\n",
      "Batch 5500 Loss: 0.4001\n",
      "Batch 5600 Loss: 0.1379\n",
      "Batch 5700 Loss: 0.3008\n",
      "Batch 5800 Loss: 0.4515\n",
      "Batch 5900 Loss: 0.2009\n",
      "Batch 6000 Loss: 0.2746\n",
      "Batch 6100 Loss: 0.2645\n",
      "Batch 6200 Loss: 0.2670\n",
      "Batch 6300 Loss: 0.2990\n",
      "Batch 6400 Loss: 0.1950\n",
      "Batch 6500 Loss: 0.2435\n",
      "Batch 6600 Loss: 0.2799\n",
      "Batch 6700 Loss: 0.3185\n",
      "Epoch 1, Loss: 0.2779\n",
      "Validation Accuracy: 0.8800, F1 Score: 0.8799\n",
      "Batch 0 Loss: 0.1825\n",
      "Batch 100 Loss: 0.3225\n",
      "Batch 200 Loss: 0.2697\n",
      "Batch 300 Loss: 0.2445\n",
      "Batch 400 Loss: 0.2601\n",
      "Batch 500 Loss: 0.3630\n",
      "Batch 600 Loss: 0.1331\n",
      "Batch 700 Loss: 0.2987\n",
      "Batch 800 Loss: 0.3091\n",
      "Batch 900 Loss: 0.3435\n",
      "Batch 1000 Loss: 0.3298\n",
      "Batch 1100 Loss: 0.3629\n",
      "Batch 1200 Loss: 0.1264\n",
      "Batch 1300 Loss: 0.3569\n",
      "Batch 1400 Loss: 0.1343\n",
      "Batch 1500 Loss: 0.2666\n",
      "Batch 1600 Loss: 0.2241\n",
      "Batch 1700 Loss: 0.3186\n",
      "Batch 1800 Loss: 0.3117\n",
      "Batch 1900 Loss: 0.3385\n",
      "Batch 2000 Loss: 0.2338\n",
      "Batch 2100 Loss: 0.5790\n",
      "Batch 2200 Loss: 0.5416\n",
      "Batch 2300 Loss: 0.3813\n",
      "Batch 2400 Loss: 0.1146\n",
      "Batch 2500 Loss: 0.2689\n",
      "Batch 2600 Loss: 0.4899\n",
      "Batch 2700 Loss: 0.1685\n",
      "Batch 2800 Loss: 0.3326\n",
      "Batch 2900 Loss: 0.2530\n",
      "Batch 3000 Loss: 0.4006\n",
      "Batch 3100 Loss: 0.2025\n",
      "Batch 3200 Loss: 0.5066\n",
      "Batch 3300 Loss: 0.4274\n",
      "Batch 3400 Loss: 0.2862\n",
      "Batch 3500 Loss: 0.2216\n",
      "Batch 3600 Loss: 0.1918\n",
      "Batch 3700 Loss: 0.2123\n",
      "Batch 3800 Loss: 0.2910\n",
      "Batch 3900 Loss: 0.2481\n",
      "Batch 4000 Loss: 0.1989\n",
      "Batch 4100 Loss: 0.2294\n",
      "Batch 4200 Loss: 0.2553\n",
      "Batch 4300 Loss: 0.2370\n",
      "Batch 4400 Loss: 0.1070\n",
      "Batch 4500 Loss: 0.1185\n",
      "Batch 4600 Loss: 0.4428\n",
      "Batch 4700 Loss: 0.2533\n",
      "Batch 4800 Loss: 0.3232\n",
      "Batch 4900 Loss: 0.2768\n",
      "Batch 5000 Loss: 0.2995\n",
      "Batch 5100 Loss: 0.1144\n",
      "Batch 5200 Loss: 0.1774\n",
      "Batch 5300 Loss: 0.3040\n",
      "Batch 5400 Loss: 0.2055\n",
      "Batch 5500 Loss: 0.3640\n",
      "Batch 5600 Loss: 0.2148\n",
      "Batch 5700 Loss: 0.1783\n",
      "Batch 5800 Loss: 0.2615\n",
      "Batch 5900 Loss: 0.1958\n",
      "Batch 6000 Loss: 0.1541\n",
      "Batch 6100 Loss: 0.1106\n",
      "Batch 6200 Loss: 0.2961\n",
      "Batch 6300 Loss: 0.2111\n",
      "Batch 6400 Loss: 0.3355\n",
      "Batch 6500 Loss: 0.1385\n",
      "Batch 6600 Loss: 0.2112\n",
      "Batch 6700 Loss: 0.2743\n",
      "Epoch 2, Loss: 0.2524\n",
      "Validation Accuracy: 0.8869, F1 Score: 0.8869\n",
      "Batch 0 Loss: 0.2381\n",
      "Batch 100 Loss: 0.5103\n",
      "Batch 200 Loss: 0.0996\n",
      "Batch 300 Loss: 0.1544\n",
      "Batch 400 Loss: 0.2823\n",
      "Batch 500 Loss: 0.2996\n",
      "Batch 600 Loss: 0.1385\n",
      "Batch 700 Loss: 0.1098\n",
      "Batch 800 Loss: 0.3222\n",
      "Batch 900 Loss: 0.2988\n",
      "Batch 1000 Loss: 0.1825\n",
      "Batch 1100 Loss: 0.2314\n",
      "Batch 1200 Loss: 0.1900\n",
      "Batch 1300 Loss: 0.2912\n",
      "Batch 1400 Loss: 0.1623\n",
      "Batch 1500 Loss: 0.1787\n",
      "Batch 1600 Loss: 0.1665\n",
      "Batch 1700 Loss: 0.1453\n",
      "Batch 1800 Loss: 0.4488\n",
      "Batch 1900 Loss: 0.3492\n",
      "Batch 2000 Loss: 0.2367\n",
      "Batch 2100 Loss: 0.1918\n",
      "Batch 2200 Loss: 0.2992\n",
      "Batch 2300 Loss: 0.2918\n",
      "Batch 2400 Loss: 0.2254\n",
      "Batch 2500 Loss: 0.1099\n",
      "Batch 2600 Loss: 0.2287\n",
      "Batch 2700 Loss: 0.3530\n",
      "Batch 2800 Loss: 0.2570\n",
      "Batch 2900 Loss: 0.0924\n",
      "Batch 3000 Loss: 0.2205\n",
      "Batch 3100 Loss: 0.3882\n",
      "Batch 3200 Loss: 0.3144\n",
      "Batch 3300 Loss: 0.2104\n",
      "Batch 3400 Loss: 0.3836\n",
      "Batch 3500 Loss: 0.1072\n",
      "Batch 3600 Loss: 0.3201\n",
      "Batch 3700 Loss: 0.1851\n",
      "Batch 3800 Loss: 0.4193\n",
      "Batch 3900 Loss: 0.2023\n",
      "Batch 4000 Loss: 0.2286\n",
      "Batch 4100 Loss: 0.2572\n",
      "Batch 4200 Loss: 0.1920\n",
      "Batch 4300 Loss: 0.2630\n",
      "Batch 4400 Loss: 0.2826\n",
      "Batch 4500 Loss: 0.3132\n",
      "Batch 4600 Loss: 0.2285\n",
      "Batch 4700 Loss: 0.3139\n",
      "Batch 4800 Loss: 0.3750\n",
      "Batch 4900 Loss: 0.3353\n",
      "Batch 5000 Loss: 0.2943\n",
      "Batch 5100 Loss: 0.2896\n",
      "Batch 5200 Loss: 0.1451\n",
      "Batch 5300 Loss: 0.1630\n",
      "Batch 5400 Loss: 0.2124\n",
      "Batch 5500 Loss: 0.3896\n",
      "Batch 5600 Loss: 0.1473\n",
      "Batch 5700 Loss: 0.1484\n",
      "Batch 5800 Loss: 0.3638\n",
      "Batch 5900 Loss: 0.2161\n",
      "Batch 6000 Loss: 0.2444\n",
      "Batch 6100 Loss: 0.1738\n",
      "Batch 6200 Loss: 0.1979\n",
      "Batch 6300 Loss: 0.1600\n",
      "Batch 6400 Loss: 0.2691\n",
      "Batch 6500 Loss: 0.4348\n",
      "Batch 6600 Loss: 0.1677\n",
      "Batch 6700 Loss: 0.1848\n",
      "Epoch 3, Loss: 0.2324\n",
      "Validation Accuracy: 0.8854, F1 Score: 0.8854\n",
      "Batch 0 Loss: 0.1374\n",
      "Batch 100 Loss: 0.2042\n",
      "Batch 200 Loss: 0.1277\n",
      "Batch 300 Loss: 0.3330\n",
      "Batch 400 Loss: 0.1840\n",
      "Batch 500 Loss: 0.1308\n",
      "Batch 600 Loss: 0.3927\n",
      "Batch 700 Loss: 0.3353\n",
      "Batch 800 Loss: 0.2727\n",
      "Batch 900 Loss: 0.0680\n",
      "Batch 1000 Loss: 0.2637\n",
      "Batch 1100 Loss: 0.1221\n",
      "Batch 1200 Loss: 0.1997\n",
      "Batch 1300 Loss: 0.0995\n",
      "Batch 1400 Loss: 0.2775\n",
      "Batch 1500 Loss: 0.0807\n",
      "Batch 1600 Loss: 0.1379\n",
      "Batch 1700 Loss: 0.0691\n",
      "Batch 1800 Loss: 0.2539\n",
      "Batch 1900 Loss: 0.1519\n",
      "Batch 2000 Loss: 0.1356\n",
      "Batch 2100 Loss: 0.2104\n",
      "Batch 2200 Loss: 0.3182\n",
      "Batch 2300 Loss: 0.1282\n",
      "Batch 2400 Loss: 0.1356\n",
      "Batch 2500 Loss: 0.2069\n",
      "Batch 2600 Loss: 0.2123\n",
      "Batch 2700 Loss: 0.2324\n",
      "Batch 2800 Loss: 0.1652\n",
      "Batch 2900 Loss: 0.3513\n",
      "Batch 3000 Loss: 0.3005\n",
      "Batch 3100 Loss: 0.3119\n",
      "Batch 3200 Loss: 0.1720\n",
      "Batch 3300 Loss: 0.2477\n",
      "Batch 3400 Loss: 0.2390\n",
      "Batch 3500 Loss: 0.1381\n",
      "Batch 3600 Loss: 0.2025\n",
      "Batch 3700 Loss: 0.1511\n",
      "Batch 3800 Loss: 0.0983\n",
      "Batch 3900 Loss: 0.4462\n",
      "Batch 4000 Loss: 0.2616\n",
      "Batch 4100 Loss: 0.2848\n",
      "Batch 4200 Loss: 0.4145\n",
      "Batch 4300 Loss: 0.2562\n",
      "Batch 4400 Loss: 0.1182\n",
      "Batch 4500 Loss: 0.1954\n",
      "Batch 4600 Loss: 0.0731\n",
      "Batch 4700 Loss: 0.3917\n",
      "Batch 4800 Loss: 0.0853\n",
      "Batch 4900 Loss: 0.1558\n",
      "Batch 5000 Loss: 0.1721\n",
      "Batch 5100 Loss: 0.1409\n",
      "Batch 5200 Loss: 0.5057\n",
      "Batch 5300 Loss: 0.2491\n",
      "Batch 5400 Loss: 0.3331\n",
      "Batch 5500 Loss: 0.0643\n",
      "Batch 5600 Loss: 0.1290\n",
      "Batch 5700 Loss: 0.2920\n",
      "Batch 5800 Loss: 0.1129\n",
      "Batch 5900 Loss: 0.3035\n",
      "Batch 6000 Loss: 0.2618\n",
      "Batch 6100 Loss: 0.0796\n",
      "Batch 6200 Loss: 0.2109\n",
      "Batch 6300 Loss: 0.1058\n",
      "Batch 6400 Loss: 0.0968\n",
      "Batch 6500 Loss: 0.2224\n",
      "Batch 6600 Loss: 0.2073\n",
      "Batch 6700 Loss: 0.1616\n",
      "Epoch 4, Loss: 0.2139\n",
      "Validation Accuracy: 0.8905, F1 Score: 0.8905\n",
      "Batch 0 Loss: 0.1337\n",
      "Batch 100 Loss: 0.1793\n",
      "Batch 200 Loss: 0.3694\n",
      "Batch 300 Loss: 0.0834\n",
      "Batch 400 Loss: 0.1779\n",
      "Batch 500 Loss: 0.3367\n",
      "Batch 600 Loss: 0.1192\n",
      "Batch 700 Loss: 0.1329\n",
      "Batch 800 Loss: 0.1693\n",
      "Batch 900 Loss: 0.0869\n",
      "Batch 1000 Loss: 0.0898\n",
      "Batch 1100 Loss: 0.4291\n",
      "Batch 1200 Loss: 0.3573\n",
      "Batch 1300 Loss: 0.3386\n",
      "Batch 1400 Loss: 0.2736\n",
      "Batch 1500 Loss: 0.2381\n",
      "Batch 1600 Loss: 0.2916\n",
      "Batch 1700 Loss: 0.3052\n",
      "Batch 1800 Loss: 0.1812\n",
      "Batch 1900 Loss: 0.1879\n",
      "Batch 2000 Loss: 0.1725\n",
      "Batch 2100 Loss: 0.1937\n",
      "Batch 2200 Loss: 0.3251\n",
      "Batch 2300 Loss: 0.3060\n",
      "Batch 2400 Loss: 0.1696\n",
      "Batch 2500 Loss: 0.0606\n",
      "Batch 2600 Loss: 0.2205\n",
      "Batch 2700 Loss: 0.0739\n",
      "Batch 2800 Loss: 0.3420\n",
      "Batch 2900 Loss: 0.1457\n",
      "Batch 3000 Loss: 0.0455\n",
      "Batch 3100 Loss: 0.0779\n",
      "Batch 3200 Loss: 0.1359\n",
      "Batch 3300 Loss: 0.3380\n",
      "Batch 3400 Loss: 0.1380\n",
      "Batch 3500 Loss: 0.2793\n",
      "Batch 3600 Loss: 0.1112\n",
      "Batch 3700 Loss: 0.1555\n",
      "Batch 3800 Loss: 0.1508\n",
      "Batch 3900 Loss: 0.1898\n",
      "Batch 4000 Loss: 0.1043\n",
      "Batch 4100 Loss: 0.2395\n",
      "Batch 4200 Loss: 0.2144\n",
      "Batch 4300 Loss: 0.2439\n",
      "Batch 4400 Loss: 0.0725\n",
      "Batch 4500 Loss: 0.2708\n",
      "Batch 4600 Loss: 0.2313\n",
      "Batch 4700 Loss: 0.3030\n",
      "Batch 4800 Loss: 0.1007\n",
      "Batch 4900 Loss: 0.2497\n",
      "Batch 5000 Loss: 0.0440\n",
      "Batch 5100 Loss: 0.4433\n",
      "Batch 5200 Loss: 0.1698\n",
      "Batch 5300 Loss: 0.1552\n",
      "Batch 5400 Loss: 0.1847\n",
      "Batch 5500 Loss: 0.1778\n",
      "Batch 5600 Loss: 0.3890\n",
      "Batch 5700 Loss: 0.1943\n",
      "Batch 5800 Loss: 0.1819\n",
      "Batch 5900 Loss: 0.1971\n",
      "Batch 6000 Loss: 0.1108\n",
      "Batch 6100 Loss: 0.2827\n",
      "Batch 6200 Loss: 0.0651\n",
      "Batch 6300 Loss: 0.5102\n",
      "Batch 6400 Loss: 0.1621\n",
      "Batch 6500 Loss: 0.2353\n",
      "Batch 6600 Loss: 0.2249\n",
      "Batch 6700 Loss: 0.0998\n",
      "Epoch 5, Loss: 0.1969\n",
      "Validation Accuracy: 0.8900, F1 Score: 0.8900\n",
      "Batch 0 Loss: 0.1942\n",
      "Batch 100 Loss: 0.0858\n",
      "Batch 200 Loss: 0.1588\n",
      "Batch 300 Loss: 0.2187\n",
      "Batch 400 Loss: 0.1823\n",
      "Batch 500 Loss: 0.0826\n",
      "Batch 600 Loss: 0.1556\n",
      "Batch 700 Loss: 0.1709\n",
      "Batch 800 Loss: 0.3144\n",
      "Batch 900 Loss: 0.0959\n",
      "Batch 1000 Loss: 0.1854\n",
      "Batch 1100 Loss: 0.1235\n",
      "Batch 1200 Loss: 0.1813\n",
      "Batch 1300 Loss: 0.2880\n",
      "Batch 1400 Loss: 0.1487\n",
      "Batch 1500 Loss: 0.1494\n",
      "Batch 1600 Loss: 0.1969\n",
      "Batch 1700 Loss: 0.1395\n",
      "Batch 1800 Loss: 0.1661\n",
      "Batch 1900 Loss: 0.0710\n",
      "Batch 2000 Loss: 0.1008\n",
      "Batch 2100 Loss: 0.1822\n",
      "Batch 2200 Loss: 0.1008\n",
      "Batch 2300 Loss: 0.1483\n",
      "Batch 2400 Loss: 0.1127\n",
      "Batch 2500 Loss: 0.2686\n",
      "Batch 2600 Loss: 0.0382\n",
      "Batch 2700 Loss: 0.1331\n",
      "Batch 2800 Loss: 0.2651\n",
      "Batch 2900 Loss: 0.1596\n",
      "Batch 3000 Loss: 0.4823\n",
      "Batch 3100 Loss: 0.1276\n",
      "Batch 3200 Loss: 0.1723\n",
      "Batch 3300 Loss: 0.2773\n",
      "Batch 3400 Loss: 0.1022\n",
      "Batch 3500 Loss: 0.1748\n",
      "Batch 3600 Loss: 0.2062\n",
      "Batch 3700 Loss: 0.3181\n",
      "Batch 3800 Loss: 0.2304\n",
      "Batch 3900 Loss: 0.1323\n",
      "Batch 4000 Loss: 0.0776\n",
      "Batch 4100 Loss: 0.1207\n",
      "Batch 4200 Loss: 0.1448\n",
      "Batch 4300 Loss: 0.1331\n",
      "Batch 4400 Loss: 0.1746\n",
      "Batch 4500 Loss: 0.4614\n",
      "Batch 4600 Loss: 0.1765\n",
      "Batch 4700 Loss: 0.1858\n",
      "Batch 4800 Loss: 0.2486\n",
      "Batch 4900 Loss: 0.0524\n",
      "Batch 5000 Loss: 0.0733\n",
      "Batch 5100 Loss: 0.1470\n",
      "Batch 5200 Loss: 0.1142\n",
      "Batch 5300 Loss: 0.1560\n",
      "Batch 5400 Loss: 0.0566\n",
      "Batch 5500 Loss: 0.2723\n",
      "Batch 5600 Loss: 0.3333\n",
      "Batch 5700 Loss: 0.2527\n",
      "Batch 5800 Loss: 0.1080\n",
      "Batch 5900 Loss: 0.2307\n",
      "Batch 6000 Loss: 0.1296\n",
      "Batch 6100 Loss: 0.3075\n",
      "Batch 6200 Loss: 0.2620\n",
      "Batch 6300 Loss: 0.2176\n",
      "Batch 6400 Loss: 0.4980\n",
      "Batch 6500 Loss: 0.3505\n",
      "Batch 6600 Loss: 0.1207\n",
      "Batch 6700 Loss: 0.2497\n",
      "Epoch 6, Loss: 0.1812\n",
      "Validation Accuracy: 0.8901, F1 Score: 0.8901\n",
      "Batch 0 Loss: 0.1307\n",
      "Batch 100 Loss: 0.0856\n",
      "Batch 200 Loss: 0.0873\n",
      "Batch 300 Loss: 0.0856\n",
      "Batch 400 Loss: 0.1425\n",
      "Batch 500 Loss: 0.2634\n",
      "Batch 600 Loss: 0.1980\n",
      "Batch 700 Loss: 0.1997\n",
      "Batch 800 Loss: 0.1730\n",
      "Batch 900 Loss: 0.0762\n",
      "Batch 1000 Loss: 0.2090\n",
      "Batch 1100 Loss: 0.1191\n",
      "Batch 1200 Loss: 0.1886\n",
      "Batch 1300 Loss: 0.0665\n",
      "Batch 1400 Loss: 0.1869\n",
      "Batch 1500 Loss: 0.0490\n",
      "Batch 1600 Loss: 0.2699\n",
      "Batch 1700 Loss: 0.1454\n",
      "Batch 1800 Loss: 0.0777\n",
      "Batch 1900 Loss: 0.1297\n",
      "Batch 2000 Loss: 0.1052\n",
      "Batch 2100 Loss: 0.2723\n",
      "Batch 2200 Loss: 0.1371\n",
      "Batch 2300 Loss: 0.0823\n",
      "Batch 2400 Loss: 0.0685\n",
      "Batch 2500 Loss: 0.1543\n",
      "Batch 2600 Loss: 0.1790\n",
      "Batch 2700 Loss: 0.0992\n",
      "Batch 2800 Loss: 0.2974\n",
      "Batch 2900 Loss: 0.0746\n",
      "Batch 3000 Loss: 0.1070\n",
      "Batch 3100 Loss: 0.1533\n",
      "Batch 3200 Loss: 0.2483\n",
      "Batch 3300 Loss: 0.1987\n",
      "Batch 3400 Loss: 0.0765\n",
      "Batch 3500 Loss: 0.1845\n",
      "Batch 3600 Loss: 0.2295\n",
      "Batch 3700 Loss: 0.0874\n",
      "Batch 3800 Loss: 0.1235\n",
      "Batch 3900 Loss: 0.1838\n",
      "Batch 4000 Loss: 0.0842\n",
      "Batch 4100 Loss: 0.1267\n",
      "Batch 4200 Loss: 0.1151\n",
      "Batch 4300 Loss: 0.1699\n",
      "Batch 4400 Loss: 0.0785\n",
      "Batch 4500 Loss: 0.3545\n",
      "Batch 4600 Loss: 0.1772\n",
      "Batch 4700 Loss: 0.1077\n",
      "Batch 4800 Loss: 0.1911\n",
      "Batch 4900 Loss: 0.3439\n",
      "Batch 5000 Loss: 0.2928\n",
      "Batch 5100 Loss: 0.2651\n",
      "Batch 5200 Loss: 0.1413\n",
      "Batch 5300 Loss: 0.0651\n",
      "Batch 5400 Loss: 0.1173\n",
      "Batch 5500 Loss: 0.1171\n",
      "Batch 5600 Loss: 0.0899\n",
      "Batch 5700 Loss: 0.1035\n",
      "Batch 5800 Loss: 0.3009\n",
      "Batch 5900 Loss: 0.3316\n",
      "Batch 6000 Loss: 0.1003\n",
      "Batch 6100 Loss: 0.0342\n",
      "Batch 6200 Loss: 0.1199\n",
      "Batch 6300 Loss: 0.3913\n",
      "Batch 6400 Loss: 0.2669\n",
      "Batch 6500 Loss: 0.2145\n",
      "Batch 6600 Loss: 0.3439\n",
      "Batch 6700 Loss: 0.1386\n",
      "Epoch 7, Loss: 0.1662\n",
      "Validation Accuracy: 0.8883, F1 Score: 0.8883\n",
      "Batch 0 Loss: 0.1990\n",
      "Batch 100 Loss: 0.1095\n",
      "Batch 200 Loss: 0.2470\n",
      "Batch 300 Loss: 0.1557\n",
      "Batch 400 Loss: 0.1797\n",
      "Batch 500 Loss: 0.0676\n",
      "Batch 600 Loss: 0.0991\n",
      "Batch 700 Loss: 0.1424\n",
      "Batch 800 Loss: 0.1782\n",
      "Batch 900 Loss: 0.0884\n",
      "Batch 1000 Loss: 0.0571\n",
      "Batch 1100 Loss: 0.1689\n",
      "Batch 1200 Loss: 0.1224\n",
      "Batch 1300 Loss: 0.2460\n",
      "Batch 1400 Loss: 0.1992\n",
      "Batch 1500 Loss: 0.1665\n",
      "Batch 1600 Loss: 0.0808\n",
      "Batch 1700 Loss: 0.0813\n",
      "Batch 1800 Loss: 0.1246\n",
      "Batch 1900 Loss: 0.1580\n",
      "Batch 2000 Loss: 0.0759\n",
      "Batch 2100 Loss: 0.4272\n",
      "Batch 2200 Loss: 0.1955\n",
      "Batch 2300 Loss: 0.2785\n",
      "Batch 2400 Loss: 0.4467\n",
      "Batch 2500 Loss: 0.1433\n",
      "Batch 2600 Loss: 0.0644\n",
      "Batch 2700 Loss: 0.0655\n",
      "Batch 2800 Loss: 0.1621\n",
      "Batch 2900 Loss: 0.3047\n",
      "Batch 3000 Loss: 0.1080\n",
      "Batch 3100 Loss: 0.1997\n",
      "Batch 3200 Loss: 0.2569\n",
      "Batch 3300 Loss: 0.1009\n",
      "Batch 3400 Loss: 0.1451\n",
      "Batch 3500 Loss: 0.2158\n",
      "Batch 3600 Loss: 0.2485\n",
      "Batch 3700 Loss: 0.1781\n",
      "Batch 3800 Loss: 0.0689\n",
      "Batch 3900 Loss: 0.1349\n",
      "Batch 4000 Loss: 0.1085\n",
      "Batch 4100 Loss: 0.0704\n",
      "Batch 4200 Loss: 0.1133\n",
      "Batch 4300 Loss: 0.1177\n",
      "Batch 4400 Loss: 0.3445\n",
      "Batch 4500 Loss: 0.0749\n",
      "Batch 4600 Loss: 0.0492\n",
      "Batch 4700 Loss: 0.2180\n",
      "Batch 4800 Loss: 0.0304\n",
      "Batch 4900 Loss: 0.1552\n",
      "Batch 5000 Loss: 0.2608\n",
      "Batch 5100 Loss: 0.0874\n",
      "Batch 5200 Loss: 0.0749\n",
      "Batch 5300 Loss: 0.1215\n",
      "Batch 5400 Loss: 0.1195\n",
      "Batch 5500 Loss: 0.1033\n",
      "Batch 5600 Loss: 0.1905\n",
      "Batch 5700 Loss: 0.0635\n",
      "Batch 5800 Loss: 0.1252\n",
      "Batch 5900 Loss: 0.1811\n",
      "Batch 6000 Loss: 0.1121\n",
      "Batch 6100 Loss: 0.1103\n",
      "Batch 6200 Loss: 0.0605\n",
      "Batch 6300 Loss: 0.2247\n",
      "Batch 6400 Loss: 0.1123\n",
      "Batch 6500 Loss: 0.0673\n",
      "Batch 6600 Loss: 0.1355\n",
      "Batch 6700 Loss: 0.1406\n",
      "Epoch 8, Loss: 0.1531\n",
      "Validation Accuracy: 0.8912, F1 Score: 0.8912\n",
      "Batch 0 Loss: 0.0820\n",
      "Batch 100 Loss: 0.0309\n",
      "Batch 200 Loss: 0.0833\n",
      "Batch 300 Loss: 0.1784\n",
      "Batch 400 Loss: 0.0874\n",
      "Batch 500 Loss: 0.1366\n",
      "Batch 600 Loss: 0.0600\n",
      "Batch 700 Loss: 0.0544\n",
      "Batch 800 Loss: 0.0359\n",
      "Batch 900 Loss: 0.3013\n",
      "Batch 1000 Loss: 0.1178\n",
      "Batch 1100 Loss: 0.3874\n",
      "Batch 1200 Loss: 0.1863\n",
      "Batch 1300 Loss: 0.1083\n",
      "Batch 1400 Loss: 0.2477\n",
      "Batch 1500 Loss: 0.0426\n",
      "Batch 1600 Loss: 0.2205\n",
      "Batch 1700 Loss: 0.1848\n",
      "Batch 1800 Loss: 0.1918\n",
      "Batch 1900 Loss: 0.0597\n",
      "Batch 2000 Loss: 0.2119\n",
      "Batch 2100 Loss: 0.1723\n",
      "Batch 2200 Loss: 0.0548\n",
      "Batch 2300 Loss: 0.0627\n",
      "Batch 2400 Loss: 0.0497\n",
      "Batch 2500 Loss: 0.1873\n",
      "Batch 2600 Loss: 0.1899\n",
      "Batch 2700 Loss: 0.0927\n",
      "Batch 2800 Loss: 0.1100\n",
      "Batch 2900 Loss: 0.2806\n",
      "Batch 3000 Loss: 0.1218\n",
      "Batch 3100 Loss: 0.0285\n",
      "Batch 3200 Loss: 0.2053\n",
      "Batch 3300 Loss: 0.1958\n",
      "Batch 3400 Loss: 0.1283\n",
      "Batch 3500 Loss: 0.0796\n",
      "Batch 3600 Loss: 0.1372\n",
      "Batch 3700 Loss: 0.0629\n",
      "Batch 3800 Loss: 0.2439\n",
      "Batch 3900 Loss: 0.0952\n",
      "Batch 4000 Loss: 0.0417\n",
      "Batch 4100 Loss: 0.2187\n",
      "Batch 4200 Loss: 0.0477\n",
      "Batch 4300 Loss: 0.2083\n",
      "Batch 4400 Loss: 0.1452\n",
      "Batch 4500 Loss: 0.2257\n",
      "Batch 4600 Loss: 0.0885\n",
      "Batch 4700 Loss: 0.1597\n",
      "Batch 4800 Loss: 0.1204\n",
      "Batch 4900 Loss: 0.1979\n",
      "Batch 5000 Loss: 0.1885\n",
      "Batch 5100 Loss: 0.3048\n",
      "Batch 5200 Loss: 0.0331\n",
      "Batch 5300 Loss: 0.0648\n",
      "Batch 5400 Loss: 0.0268\n",
      "Batch 5500 Loss: 0.2635\n",
      "Batch 5600 Loss: 0.2172\n",
      "Batch 5700 Loss: 0.2363\n",
      "Batch 5800 Loss: 0.1262\n",
      "Batch 5900 Loss: 0.2554\n",
      "Batch 6000 Loss: 0.1290\n",
      "Batch 6100 Loss: 0.1427\n",
      "Batch 6200 Loss: 0.1649\n",
      "Batch 6300 Loss: 0.1779\n",
      "Batch 6400 Loss: 0.1378\n",
      "Batch 6500 Loss: 0.1779\n",
      "Batch 6600 Loss: 0.1796\n",
      "Batch 6700 Loss: 0.1617\n",
      "Epoch 9, Loss: 0.1408\n",
      "Validation Accuracy: 0.8880, F1 Score: 0.8879\n",
      "Batch 0 Loss: 0.1635\n",
      "Batch 100 Loss: 0.0631\n",
      "Batch 200 Loss: 0.0353\n",
      "Batch 300 Loss: 0.0343\n",
      "Batch 400 Loss: 0.1008\n",
      "Batch 500 Loss: 0.0620\n",
      "Batch 600 Loss: 0.0365\n",
      "Batch 700 Loss: 0.0827\n",
      "Batch 800 Loss: 0.1092\n",
      "Batch 900 Loss: 0.2120\n",
      "Batch 1000 Loss: 0.0602\n",
      "Batch 1100 Loss: 0.1625\n",
      "Batch 1200 Loss: 0.0828\n",
      "Batch 1300 Loss: 0.2564\n",
      "Batch 1400 Loss: 0.0729\n",
      "Batch 1500 Loss: 0.0796\n",
      "Batch 1600 Loss: 0.0661\n",
      "Batch 1700 Loss: 0.2730\n",
      "Batch 1800 Loss: 0.2258\n",
      "Batch 1900 Loss: 0.0994\n",
      "Batch 2000 Loss: 0.1762\n",
      "Batch 2100 Loss: 0.1073\n",
      "Batch 2200 Loss: 0.1291\n",
      "Batch 2300 Loss: 0.1485\n",
      "Batch 2400 Loss: 0.0495\n",
      "Batch 2500 Loss: 0.1195\n",
      "Batch 2600 Loss: 0.0447\n",
      "Batch 2700 Loss: 0.1089\n",
      "Batch 2800 Loss: 0.0325\n",
      "Batch 2900 Loss: 0.0073\n",
      "Batch 3000 Loss: 0.1877\n",
      "Batch 3100 Loss: 0.1605\n",
      "Batch 3200 Loss: 0.3050\n",
      "Batch 3300 Loss: 0.0802\n",
      "Batch 3400 Loss: 0.2859\n",
      "Batch 3500 Loss: 0.0837\n",
      "Batch 3600 Loss: 0.0140\n",
      "Batch 3700 Loss: 0.0783\n",
      "Batch 3800 Loss: 0.1137\n",
      "Batch 3900 Loss: 0.2453\n",
      "Batch 4000 Loss: 0.1330\n",
      "Batch 4100 Loss: 0.1918\n",
      "Batch 4200 Loss: 0.0318\n",
      "Batch 4300 Loss: 0.1730\n",
      "Batch 4400 Loss: 0.1332\n",
      "Batch 4500 Loss: 0.1066\n",
      "Batch 4600 Loss: 0.0501\n",
      "Batch 4700 Loss: 0.1157\n",
      "Batch 4800 Loss: 0.1591\n",
      "Batch 4900 Loss: 0.1276\n",
      "Batch 5000 Loss: 0.2846\n",
      "Batch 5100 Loss: 0.0316\n",
      "Batch 5200 Loss: 0.3748\n",
      "Batch 5300 Loss: 0.1528\n",
      "Batch 5400 Loss: 0.2397\n",
      "Batch 5500 Loss: 0.0866\n",
      "Batch 5600 Loss: 0.1884\n",
      "Batch 5700 Loss: 0.1215\n",
      "Batch 5800 Loss: 0.0314\n",
      "Batch 5900 Loss: 0.1239\n",
      "Batch 6000 Loss: 0.1604\n",
      "Batch 6100 Loss: 0.2951\n",
      "Batch 6200 Loss: 0.0509\n",
      "Batch 6300 Loss: 0.0419\n",
      "Batch 6400 Loss: 0.0108\n",
      "Batch 6500 Loss: 0.1057\n",
      "Batch 6600 Loss: 0.1149\n",
      "Batch 6700 Loss: 0.0618\n",
      "Epoch 10, Loss: 0.1303\n",
      "Validation Accuracy: 0.8883, F1 Score: 0.8883\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits if hasattr(outputs, 'logits') else outputs\n",
    "        \n",
    "        loss = loss_fn(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Batch {batch_idx} Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    preds, targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            logits = model(input_ids, attention_mask)\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            preds.extend(predictions.cpu().numpy())\n",
    "            targets.extend(labels.cpu().numpy())\n",
    "    \n",
    "    acc = accuracy_score(targets, preds)\n",
    "    f1 = f1_score(targets, preds, average='macro')\n",
    "    print(f\"Validation Accuracy: {acc:.4f}, F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ba4dc4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save entire model\n",
    "torch.save(model.state_dict(), 'model_weights.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d49b2b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Example dummy input for BERT-like models (batch_size=1, sequence_length=128)\n",
    "dummy_input_ids = torch.randint(0, 1000, (1, 128)).to(device)\n",
    "dummy_attention_mask = torch.ones((1, 128)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f4214f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/run/media/seono/P/Sequence/layers/attention.py:130: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  scores = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))\n"
     ]
    }
   ],
   "source": [
    "torch.onnx.export(\n",
    "    model,  # Your model\n",
    "    (dummy_input_ids, dummy_attention_mask),  # Tuple of inputs\n",
    "    \"sequence_classifier.onnx\",  # Output file\n",
    "    input_names=[\"input_ids\", \"attention_mask\"],\n",
    "    output_names=[\"logits\"],\n",
    "    dynamic_axes={\n",
    "        'input_ids': {0: 'batch_size', 1: 'sequence_length'},\n",
    "        'attention_mask': {0: 'batch_size', 1: 'sequence_length'},\n",
    "        'logits': {0: 'batch_size'}\n",
    "    },\n",
    "    opset_version=13,\n",
    "    export_params=True,\n",
    "    do_constant_folding=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1b14af01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting onnx\n",
      "  Downloading onnx-1.18.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: numpy>=1.22 in /home/seono/anaconda3/lib/python3.12/site-packages (from onnx) (1.26.4)\n",
      "Requirement already satisfied: protobuf>=4.25.1 in /home/seono/anaconda3/lib/python3.12/site-packages (from onnx) (4.25.3)\n",
      "Requirement already satisfied: typing_extensions>=4.7.1 in /home/seono/anaconda3/lib/python3.12/site-packages (from onnx) (4.12.2)\n",
      "Downloading onnx-1.18.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: onnx\n",
      "Successfully installed onnx-1.18.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c289feac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
